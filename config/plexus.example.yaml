###############################################################################
# Plexus Configuration Documentation
###############################################################################
#
# This file configures the Plexus 2 gateway, defining how requests are routed
# and transformed between different LLM providers.
#
# 1. PROVIDERS SECTION
# --------------------
# Define your upstream AI providers here.
#
#   type:         The transformer architecture used by the provider. 
#                 Common values: chat, messages, gemini.
#   display_name: A friendly name for logging and metadata.
#   api_base_url: The root endpoint for the provider's API.
#   api_key:      Your authentication token for the provider.
#   models:       A list of raw model identifiers supported by this provider.
#                 OR a map of model names to configuration (e.g. pricing).
#   headers:      (Optional) Custom HTTP headers to include in every request.
#
# 2. MODELS SECTION (ALIASES)
# ---------------------------
# Define friendly "Model Aliases" that your clients will use.
#
#   selector:     (Optional) The strategy to select a target.
#                 Options: 'random' (default), 'cost', 'performance', 'latency'.
#                 If multiple targets are available, Plexus will prioritize
#                 selection based on the defined 'selector'.
#                   - random: Randomly selects a healthy target.
#                   - cost: Selects the target with the lowest configured cost.
#                   - performance: Selects the target with the highest average tokens/sec.
#                   - latency: Selects the target with the lowest average time-to-first-token.
#
# 3. KEYS SECTION
# ---------------
# Define valid API keys for accessing the Plexus gateway.
#
#   secret:       The actual bearer token string clients must provide.
#   comment:      (Optional) Description or owner of the key.
#
###############################################################################

# [REQUIRED] Admin Key for Dashboard and Management API Access
adminKey: "change-me-to-a-secure-admin-password"

providers:
  # Standard OpenAI Configuration
  openai:
    type: chat
    display_name: OpenAI
    api_base_url: https://api.openai.com/v1
    api_key: "your-openai-api-key-here"
    models:
      - gpt-4o
      - gpt-4o-mini
      - o1-preview

  # Standard Anthropic Configuration
  anthropic:
    type: messages
    display_name: Anthropic Claude
    api_base_url: https://api.anthropic.com/v1
    api_key: "your-anthropic-api-key-here"
    models:
      - claude-3-5-sonnet-latest
      - claude-3-5-haiku-latest
      - claude-3-opus-latest

  # Standard Google Gemini Configuration
  gemini:
    type: gemini
    display_name: Google Gemini
    api_base_url: https://generativelanguage.googleapis.com
    api_key: "your-gemini-api-key-here"
    models:
      - gemini-1.5-pro
      - gemini-1.5-flash
      - gemini-2.0-flash-exp

  # Example with Pricing Configuration
  # Pricing is per 1M tokens.
  openai_cost_tracked:
    type: chat
    display_name: OpenAI (Tracked)
    api_base_url: https://api.openai.com/v1
    api_key: "your-openai-api-key-here"
    models:
      gpt-4o:
        pricing:
          source: simple
          input: 2.50
          output: 10.00
      gpt-4o-mini:
        pricing:
          source: simple
          input: 0.15
          output: 0.60

  # Example with Tiered Pricing Configuration (Defined Strategy)
  # Pricing changes based on input token usage volume.
  tiered_pricing_example:
    type: chat
    display_name: Tiered Pricing Provider
    api_base_url: https://api.example.com/v1
    api_key: "your-api-key"
    models:
      tiered-model-v1:
        pricing:
          source: defined
          range:
            # Tier 1: 0 - 1M tokens
            - lower_bound: 0
              upper_bound: 1000000
              input_per_m: 5.00
              output_per_m: 15.00
            # Tier 2: > 1M tokens
            - lower_bound: 1000001
              upper_bound: .inf
              input_per_m: 4.00
              output_per_m: 12.00

  # Example of a Chat-compatible provider (e.g. Together, DeepSeek, Groq)
  deepseek:
    type: chat
    display_name: DeepSeek
    api_base_url: https://api.deepseek.com
    api_key: "your-deepseek-api-key-here"
    models:
      - deepseek-chat
      - deepseek-reasoner

  # Example with custom headers and OpenRouter pricing lookup
  openrouter:
    type: chat
    display_name: OpenRouter
    api_base_url: https://openrouter.ai/api/v1
    api_key: "your-openrouter-key-here"
    # Optional global discount for this provider (e.g., 5% off)
    discount: 0.05
    models:
      google/gemini-pro-1.5:
        pricing:
          source: openrouter
          slug: google/gemini-pro-1.5
      # Example with discount (e.g., 10% off list price)
      anthropic/claude-3.5-sonnet:
        pricing:
          source: openrouter
          slug: anthropic/claude-3.5-sonnet
          discount: 0.1
    headers:
      "HTTP-Referer": "https://your-app.com"
      "X-Title": "My Plexus App"

models:
  # Basic alias routing to a single target
  smart-model:
    targets:
      - provider: anthropic
        model: claude-3-5-sonnet-latest

  # Alias with additional aliases
  # Requests to 'gpt-4' or 'gpt-4-turbo' will also route here
  gpt-4-wrapper:
    additional_aliases:
      - gpt-4
      - gpt-4-turbo
    targets:
      - provider: openai
        model: gpt-4o

  # Alias routing to a Chat-compatible backend
  fast-model:
    targets:
      - provider: openai
        model: gpt-4o-mini

  # Reasoning model alias
  reasoning-model:
    targets:
      - provider: deepseek
        model: deepseek-reasoner

  # Example of Load Balancing with Explicit Selector
  # Requests to 'balanced-model' will distribute randomly across these two targets
  balanced-model:
    selector: random
    targets:
      - provider: openai
        model: gpt-4o
      - provider: anthropic
        model: claude-3-5-sonnet-latest

  # Example using Performance Selector (Fastest TPS)
  fastest-throughput-model:
    selector: performance
    targets:
      - provider: groq
        model: llama-3.1-70b
      - provider: openai
        model: gpt-4o-mini

  # Example using Latency Selector (Lowest TTFT)
  lowest-latency-model:
    selector: latency
    targets:
      - provider: anthropic
        model: claude-3-5-haiku-latest
      - provider: openai
        model: gpt-4o-mini

keys:
  # Example API Key for accessing Plexus
  my-app-key:
    secret: "sk-plexus-example-key-123"
    comment: "Key for the main application"
  
  # Another key for testing
  test-key:
    secret: "sk-plexus-test-key-456"
    comment: "CI/CD Test Key"