###############################################################################
# Plexus Configuration Documentation
###############################################################################
#
# This file configures the Plexus 2 gateway, defining how requests are routed
# and transformed between different LLM providers.
#
# 1. PROVIDERS SECTION
# --------------------
# Define your upstream AI providers here.
#
#   display_name: A friendly name for logging and metadata.
#   api_base_url: The root endpoint for the provider's API.
#                 Plexus automatically infers the API type (chat/messages/gemini/embeddings) from this URL:
#                 - URLs containing 'anthropic.com' are treated as 'messages' (Anthropic format)
#                 - URLs containing 'generativelanguage.googleapis.com' are treated as 'gemini'
#                 - All other URLs default to 'chat' (OpenAI-compatible format)
#
#                 For providers that support multiple formats, use a map:
#                 api_base_url:
#                   chat: https://api.example.com/v1
#                   messages: https://api.example.com/anthropic/v1
#                   embeddings: https://api.example.com/v1
#
#   api_key:      Your authentication token for the provider (required).
#   models:       A list of raw model identifiers supported by this provider.
#                 OR a map of model names to configuration (e.g. pricing, type).
#                 For embeddings models, specify type: embeddings in the model config.
#   headers:      (Optional) Custom HTTP headers to include in every request.
#
# 2. MODELS SECTION (ALIASES)
# ---------------------------
# Define friendly "Model Aliases" that your clients will use.
#
#   type:         (Optional) 'chat' (default) or 'embeddings'. Determines which
#                 API endpoints can access this model.
#   selector:     (Optional) The strategy to select a target.
#                 Options: 'random' (default), 'cost', 'performance', 'latency', 'in_order'.
#                 If multiple targets are available, Plexus will prioritize
#                 selection based on the defined 'selector'.
#                   - random: Randomly selects a healthy target.
#                   - in_order: Selects providers in the order defined. Falls back to next if current is unhealthy.
#                   - cost: Selects the target with the lowest configured cost.
#                   - performance: Selects the target with the highest average tokens/sec.
#                   - latency: Selects the target with the lowest average time-to-first-token.
#
# 3. KEYS SECTION
# ---------------
# Define valid API keys for accessing the Plexus gateway.
#
#   secret:       The actual bearer token string clients must provide.
#   comment:      (Optional) Description or owner of the key.
#
###############################################################################

# [REQUIRED] Admin Key for Dashboard and Management API Access
adminKey: "change-me-to-a-secure-admin-password"

providers:
  # Standard OpenAI Configuration
  # API type is automatically inferred as 'chat' from the URL
  openai:
    display_name: OpenAI
    api_base_url: https://api.openai.com/v1
    api_key: "your-openai-api-key-here"
    models:
      - gpt-4o
      - gpt-4o-mini
      - o1-preview
      - text-embedding-3-small  # Embeddings model

  # Standard Anthropic Configuration
  # API type is automatically inferred as 'messages' from the URL
  anthropic:
    display_name: Anthropic Claude
    api_base_url: https://api.anthropic.com/v1
    api_key: "your-anthropic-api-key-here"
    models:
      - claude-3-5-sonnet-latest
      - claude-3-5-haiku-latest
      - claude-3-opus-latest

  # Standard Google Gemini Configuration
  # API type is automatically inferred as 'gemini' from the URL
  gemini:
    display_name: Google Gemini
    api_base_url: https://generativelanguage.googleapis.com
    api_key: "your-gemini-api-key-here"
    models:
      - gemini-1.5-pro
      - gemini-1.5-flash
      - gemini-2.0-flash-exp

  # OAuth-backed providers (pi-ai)
  # Requires OAuth login via: npx @mariozechner/pi-ai login <provider>
  anthropic-oauth:
    display_name: Anthropic Claude (OAuth)
    api_base_url: oauth://
    api_key: oauth
    oauth_provider: anthropic
    oauth_account: work
    quota_checker:
      type: claude-code
      enabled: true
      intervalMinutes: 10
    models:
      - claude-3-5-sonnet-20241022
      - claude-3-5-haiku-20241022
      - claude-3-opus-20240229
      - claude-sonnet-4-20250514

  codex:
    display_name: OpenAI Codex
    api_base_url: oauth://
    api_key: oauth
    oauth_provider: openai-codex
    oauth_account: work
    quota_checker:
      type: openai-codex
      enabled: true
      intervalMinutes: 10
    models:
      - gpt-5-mini
      - gpt-5
      - gpt-5-preview

  antigravity:
    display_name: Google Antigravity
    api_base_url: oauth://
    api_key: oauth
    models:
      - gemini-3-flash
      - gemini-2.5-flash
      - claude-3-5-sonnet-20241022
      - gpt-oss-70b

  github-copilot:
    display_name: GitHub Copilot
    api_base_url: oauth://
    api_key: oauth
    models:
      - gpt-4o
      - gpt-4o-mini
      - o1-preview
      - claude-3-5-sonnet-20241022

  gemini-cli:
    display_name: Google Gemini CLI
    api_base_url: oauth://
    api_key: oauth
    models:
      - gemini-2.5-flash
      - gemini-2.0-flash

  # Example with Pricing Configuration
  # Pricing is per 1M tokens.
  openai_cost_tracked:
    display_name: OpenAI (Tracked)
    api_base_url: https://api.openai.com/v1
    api_key: "your-openai-api-key-here"
    models:
      gpt-4o:
        pricing:
          source: simple
          input: 2.50
          output: 10.00
      gpt-4o-mini:
        pricing:
          source: simple
          input: 0.15
          output: 0.60

  # Example with Tiered Pricing Configuration (Defined Strategy)
  # Pricing changes based on input token usage volume.
  tiered_pricing_example:
    display_name: Tiered Pricing Provider
    api_base_url: https://api.example.com/v1
    api_key: "your-api-key"
    models:
      tiered-model-v1:
        pricing:
          source: defined
          range:
            # Tier 1: 0 - 1M tokens
            - lower_bound: 0
              upper_bound: 1000000
              input_per_m: 5.00
              output_per_m: 15.00
            # Tier 2: > 1M tokens
            - lower_bound: 1000001
              upper_bound: .inf
              input_per_m: 4.00
              output_per_m: 12.00

  # Example of a Chat-compatible provider (e.g. Together, DeepSeek, Groq)
  # API type is automatically inferred as 'chat'
  deepseek:
    display_name: DeepSeek
    api_base_url: https://api.deepseek.com
    api_key: "your-deepseek-api-key-here"
    models:
      - deepseek-chat
      - deepseek-reasoner

  # Example embeddings provider
  # Supports OpenAI-compatible embeddings endpoint
  voyage:
    display_name: Voyage AI
    api_base_url: https://api.voyageai.com/v1
    api_key: "your-voyage-api-key-here"
    models:
      voyage-3:
        type: embeddings
        pricing:
          source: simple
          input: 0.00006
          output: 0

  # Example of a multi-protocol provider
  # This provider supports both OpenAI (chat) and Anthropic (messages) formats
  # Use a map to specify different URLs for each format
  synthetic:
    display_name: Synthetic Provider
    # Map API types to specific base URLs - types are inferred from the keys
    api_base_url:
      chat: https://api.synthetic.new/openai/v1
      messages: https://api.synthetic.new/messages/v1
    api_key: "your-synthetic-key"
    quota_checker:
      type: synthetic
      enabled: true
      intervalMinutes: 30
    models:
      # This model can be accessed via both formats
      "hf:MiniMaxAI/MiniMax-M2.1":
        access_via: ["chat", "messages"]
      # This model only supports the messages format
      "legacy-model":
        access_via: ["messages"]

  # Example with custom headers and OpenRouter pricing lookup
  openrouter:
    display_name: OpenRouter
    api_base_url: https://openrouter.ai/api/v1
    api_key: "your-openrouter-key-here"
    # Optional global discount for this provider (e.g., 5% off)
    discount: 0.05
    models:
      google/gemini-pro-1.5:
        pricing:
          source: openrouter
          slug: google/gemini-pro-1.5
      # Example with discount (e.g., 10% off list price)
      anthropic/claude-3.5-sonnet:
        pricing:
          source: openrouter
          slug: anthropic/claude-3.5-sonnet
          discount: 0.1
    headers:
      "HTTP-Referer": "https://your-app.com"

models:
  # Basic alias routing to Anthropic
  smart-model:
    targets:
      - provider: anthropic
        model: claude-3-5-sonnet-latest

  # Alias with additional aliases
  # Requests to 'gpt-4' or 'gpt-4-turbo' will also route here
  gpt-4-wrapper:
    additional_aliases:
      - gpt-4
      - gpt-4-turbo
    targets:
      - provider: openai
        model: gpt-4o

  # Alias routing to a Chat-compatible backend
  fast-model:
    targets:
      - provider: openai
        model: gpt-4o-mini

  # Reasoning model alias
  reasoning-model:
    targets:
      - provider: deepseek
        model: deepseek-reasoner

  # Example of Load Balancing with Explicit Selector
  # Requests to 'balanced-model' will distribute randomly across these two targets
  balanced-model:
    selector: random
    # Optional: prioritizing native API matching over the selector
    # If a client sends an Anthropic request, Plexus will prefer the 'anthropic' provider
    priority: api_match
    targets:
      - provider: openai
        model: gpt-4o
      - provider: anthropic
        model: claude-3-5-sonnet-latest

  # Example using Cost Selector (Cheapest Provider)
  cheapest-model:
    selector: cost
    targets:
      - provider: openai_cost_tracked
        model: gpt-4o-mini
      - provider: deepseek
        model: deepseek-chat

  # Example using Performance Selector (Fastest TPS)
  fastest-throughput-model:
    selector: performance
    targets:
      - provider: openai
        model: gpt-4o
      - provider: openai
        model: gpt-4o-mini

  # Example using Latency Selector (Lowest TTFT)
  lowest-latency-model:
    selector: latency
    targets:
      - provider: anthropic
        model: claude-3-5-haiku-latest
      - provider: openai
        model: gpt-4o-mini

  # Example using InOrder Selector (Defined Order with Fallback)
  # Requests route to first provider, then second if first is unhealthy
  minimax-m2.1:
    selector: in_order
    targets:
      - provider: synthetic
        model: "hf:MiniMaxAI/MiniMax-M2.1"
      - provider: openai
        model: gpt-4o-mini

  # Example with disabled target
  # The first target is disabled, so routing will skip it
  example-with-disabled-target:
    selector: cost
    targets:
      - provider: synthetic
        model: "hf:MiniMaxAI/MiniMax-M2.1"
        enabled: false  # This target is temporarily disabled
      - provider: openai
        model: gpt-4o
        enabled: true   # Optional: true is the default
      - provider: anthropic
        model: claude-3-5-sonnet-latest
        # enabled field omitted - defaults to true

  # Embeddings model alias example
  # Type is set to 'embeddings' so it's only accessible via /v1/embeddings
  # Uses cost selector to choose cheapest embeddings provider
  embeddings-model:
    type: embeddings
    selector: cost
    targets:
      - provider: openai
        model: text-embedding-3-small
      - provider: voyage
        model: voyage-3

keys:
  # Example API Key for accessing Plexus
  my-app-key:
    secret: "sk-plexus-example-key-123"
    comment: "Key for the main application"
  
  # Another key for testing
  test-key:
    secret: "sk-plexus-test-key-456"
    comment: "CI/CD Test Key"
